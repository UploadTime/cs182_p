{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Coding: Logistic Regression.\n",
    "  Please export this jupyter notebook as PDF, and hand in .pdf (with writing part) file.\n",
    "\n",
    "In this part of homework, you need to implement Logistic Regression using Python in this jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Preparation before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part loads the necessary libraries and dataset. You are only required to do the normalization by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the required libraries. You need to implement them in first.\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset including features and binary labels\n",
    "data = load_breast_cancer().data\n",
    "target = load_breast_cancer().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of features and labels\n",
    "data.shape,target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into train and test sets 2:1 with certain random seed.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing data (by yourself)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A useful trick before training is to normalize all features to have mean 0 and unit variance first.\n",
    "# Please implement this by yourself rather than use sklearn.preprocessing.StandardScaler as the comment below. \n",
    "\n",
    "\"\"\"\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\"\"\"\n",
    "#Your codes below:\n",
    "for i in range(X_train.shape[1]):\n",
    "    for j in range(X_train.shape[0]):\n",
    "        u = np.mean(X_train[j][i])\n",
    "        s = np.std(X_train[j][i])\n",
    "    for k in range(len(X_train)):\n",
    "        X_train[k][i] = (X_train[k][i] - u)/s\n",
    "    for k in range(len(X_test)):\n",
    "        X_test[k][i] = (X_test[k][i] - u)/s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions are given below, you are free to use them or not in parts below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict y of x with current weights\n",
    "def predict(x,w):\n",
    "    y_pred=[]\n",
    "    for i in range(len(x)):\n",
    "        y = (np.asscalar(1/(1+np.exp(-(np.dot(w,x[i]))))))\n",
    "        if y<0.5:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calulate TPR,FPR,TNR and FNR to be included in confusion matrix\n",
    "def find_rates(mat):\n",
    "    mat2=[]\n",
    "    mat2.append((mat[0,0]))\n",
    "    mat2.append((mat[1,0]))\n",
    "    mat2.append((mat[0,1]))\n",
    "    mat2.append((mat[1,1]))\n",
    "    mat2=np.reshape(mat2,(2,2))\n",
    "    mat2 = pd.DataFrame(mat2,columns=[0,1],index=[0,1])\n",
    "    mat2.index.name = 'Predicted'\n",
    "    mat2.columns.name = 'Actual'\n",
    "    return mat2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implement Logistic Regression using sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you are firstly given an example Sklearn implementation of logistic regression. Play with them and then you should:\n",
    "1.  Explain the parameters and their effects in LogisticRegression().\n",
    "2.  Try different settings of parameters and show its performance as the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read official document from https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression using sklearn\n",
    "LRexample = LogisticRegression(penalty = 'l2', C=0.1, solver = 'liblinear')\n",
    "LRexample.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_sklearn = LRexample.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels of ground-truth on test set.\n",
    "np.unique(y_test,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels produced by LR model on test set.\n",
    "np.unique(y_pred_sklearn,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true-negative, false-negative, false-negative, true-positive\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_sklearn).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_test = find_rates(confusion_matrix(y_test, y_pred_sklearn))\n",
    "\n",
    "fig=plt.figure(figsize=(8,6))\n",
    "plt.title('Confusion Matrix for test data using sklearn Logistic Regression')\n",
    "sns.heatmap(mat_test,annot=True,fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRexample.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the parameters and their effects in LogisticRegression( ) in the Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalty: \n",
    "Penalty item, str type, optional parameters are l1 and l2, default is l2.\n",
    "The L1G specification assumes that the parameters of the model satisfy the Laplace distribution, and the L2 assumes that the model parameters satisfy the Gaussian distribution.\n",
    "# C:\n",
    "The reciprocal of the regularization coefficient $\\lemda$, float type, defaults to 1.0.\n",
    "# solver:\n",
    "The optimization algorithm selects parameters, and there are only five optional parameters, namely newton-cg, lbfgs, liblinear, sag, saga. The default is liblinear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different settings of Sklearn implementation of logistic regression and show the performance as the example above. Write your codes below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = []\n",
    "l2 = []\n",
    "l1test = []\n",
    "l2test = []\n",
    "for i in np.linspace(0.05,3,59):\n",
    "    lrl1 = LogisticRegression(penalty=\"l1\",solver=\"liblinear\",C=i)\n",
    "    lrl2 = LogisticRegression(penalty=\"l2\",solver=\"liblinear\",C=i)\n",
    "    lrl1 = lrl1.fit(X_train,y_train)\n",
    "    l1.append(lrl1.score(lrl1.predict(X_train),y_train))\n",
    "    l1test.append(lrl1.score(lrl1.predict(X_test),y_test))\n",
    "    lrl2 = lrl2.fit(X_train,y_train)\n",
    "    l2.append(lrl2.score(lrl2.predict(X_train),y_train))\n",
    "    l2test.append(lrl2.score(lrl2.predict(X_test),y_test))\n",
    "\n",
    "graph = [l1,l2,l1test,l2test]\n",
    "color = [\"green\",\"black\",\"lightgreen\",\"gray\"]\n",
    "label = [\"L1\",\"L2\",\"L1test\",\"L2test\"]\n",
    "\n",
    "for i in range(len(graph)):\n",
    "    plt.plot(np.linspace(0.05,3,59),graph[i],color[i],label=label[i])\n",
    "\n",
    "plt.legend(loc=4) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = LogisticRegression(penalty = \"none\", solver = \"lbfgs\", C = 1.0)\n",
    "lr1 = lr1.fit(X_train, y_train)\n",
    "print(lr1.score(X_test, y_test))\n",
    "y_pred_sklearn1 = lr1.perdict(X_test)\n",
    "mat_test = find_rates(confusion_matrix(y_test, y_pred_sklearn1))\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.title('Confusion Matrix for test data using sklearn Logistic Regression')\n",
    "sns.heatmap(mat_test,annot=True,fmt='g')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LogisticRegression(penalty = \"l2\", solver = \"sag\", C = 0.1)\n",
    "lr2 = lr2.fit(X_train, y_train)\n",
    "print(lr2.score(X_test, y_test))\n",
    "y_pred_sklearn2 = lr2.perdict(X_test)\n",
    "mat_test = find_rates(confusion_matrix(y_test, y_pred_sklearn2))\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.title('Confusion Matrix for test data using sklearn Logistic Regression')\n",
    "sns.heatmap(mat_test,annot=True,fmt='g')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr3 = LogisticRegression(penalty = \"l2\", solver = \"newton-cg\", C = 1)\n",
    "lr3 = lr3.fit(X_train, y_train)\n",
    "print(lr3.score(X_test, y_test))\n",
    "y_pred_sklearn3 = lr3.perdict(X_test)\n",
    "mat_test = find_rates(confusion_matrix(y_test, y_pred_sklearn3))\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.title('Confusion Matrix for test data using sklearn Logistic Regression')\n",
    "sns.heatmap(mat_test,annot=True,fmt='g')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement Logistic Regression without using its library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you need to implement Logistic regression model using Batch Gradient Descent and Stochastic Gradient Descent by yourself. The hyperparameters of the two algorithms are given and recommended. Notice that with given hyperparameters and random seeds, the weights obtained by BGD and SGD with momentum should be unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1: Implement logistic regression using Batch-GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decribe the Batch-GD alogrithm in the Markdown cell below. You are free to use mathematical derivation or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "At each iteration, train all the samples and update weights. The initilization point should be set to all-zero vector.\n",
    "\"\"\"\n",
    "from datetime import date\n",
    "n_iter=50  # number of iterations\n",
    "reg=0.01   # regularization parameter lambda\n",
    "r=0.1      # learning rate\n",
    "sample_size=X_train.shape[0]    # batch size for BGD\n",
    "N=X_train.shape[0]\n",
    "\n",
    "def s(x):\n",
    "    if  x >= 0:\n",
    "        return 1.0/(1+np.exp(x))\n",
    "    else:\n",
    "        return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "GD = np.zeros(X_train.shape[1])\n",
    "GD0 = 0.0\n",
    "\n",
    "for j in range(n_iter):\n",
    "    #Your codes below:\n",
    "    gd = np.zeros(X_train.shape[1])\n",
    "    gd0 = 0.0\n",
    "    for i in range(N):\n",
    "        h = s(np.dot(GD, X_train[i]) + GD0)\n",
    "        gd += (y_train[i] - h) * X_train[i]/N\n",
    "        gd0 += (y_train[i] - h)/N\n",
    "    GD = GD + r * gd - r * reg * GD\n",
    "    GD0 += r * gd0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions for test datapoints\n",
    "y_pred_BGD = predict(X_test,GD,GD0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred_BGD,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw confusion matrix\n",
    "mat_test = find_rates(confusion_matrix(y_test, y_pred_BGD))\n",
    "\n",
    "fig=plt.figure(figsize=(8,6))\n",
    "plt.title('Confusion Matrix for test data using BGD Logistic Regression')\n",
    "sns.heatmap(mat_test,annot=True,fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: Implement logistic regression using SGD with momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you need to implement logistic regression using SGD method with momentum for accelerating training.\n",
    "Intuitively, the method tries to accelerate with keeping the 'momentum' by moving along a previous direction. You may find Chapter 8.3 \n",
    "helpful https://www.deeplearningbook.org/contents/optimization.html for more details with respect to SGD, momentum and\n",
    "more acceleration tricks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decribe the SGD with momentum alogrithm in the Markdown cell below. You are free to use mathematical derivation or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "At each iteration, choose 20 samples randomly and compute dJ(theta)/d(theta) among \n",
    "those 20 samples then update the vector of weights with momentum. The initilization point should be set to all-zero vector.\n",
    "\n",
    "Note that the random seed at each iteration is given, do not modify it.\n",
    "\"\"\"\n",
    "n_iter=50  # number of iterations\n",
    "reg=0.01   # regularization parameter lambda\n",
    "r=0.1      # learning rate\n",
    "momen = 0.5 # momentum rate\n",
    "sample_size=20    # sample size for SGD\n",
    "N=X_train.shape[0]\n",
    "\n",
    "w_SGD = np.zeros(X_train.shape[1])\n",
    "w_SGD0 = 0.0\n",
    "temp = w_SGD\n",
    "temp0 = w_SGD0\n",
    "\n",
    "for j in range(n_iter):\n",
    "    np.random.seed(j) \n",
    "    idx=np.random.randint(X_train.shape[0],size=sample_size) \n",
    "    # Do NOT modify codes above, especially the random code.\n",
    "    # At each iterations, choose samples from X_train, y_train, with index idx.\n",
    "    # Your codes below:\n",
    "    gd = np.zeros(X_train.shape[1])\n",
    "    gd0 = 0.0\n",
    "    for i in idx:\n",
    "        h = s(np.dot(w_SGD, X_train[i]) + w_SGD0)\n",
    "        gd += (y_train[i] - h) * X_train[i] / sample_size\n",
    "        gd0 += (y_train[i] - h) / sample_size\n",
    "    temp = momen * temp + gd - reg * w_SGD\n",
    "    w_SGD += r * temp\n",
    "    temp0 = momen * temp0 + gd0\n",
    "    w_SGD0 += r * temp0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions for test datapoints\n",
    "y_pred_SGD = predict(X_test,w_SGD,w_SGD0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred_SGD,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw confusion matrix\n",
    "mat_test = find_rates(confusion_matrix(y_test, y_pred_SGD))\n",
    "\n",
    "fig=plt.figure(figsize=(8,6))\n",
    "plt.title('Confusion Matrix for test data using SGD Logistic Regression')\n",
    "sns.heatmap(mat_test,annot=True,fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a table to show every coefficients in vector w, and compute the absolute difference between coefficients of BGD and SGD with momentum methods.\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "p = PrettyTable()\n",
    "p.title='Weights from both models'\n",
    "p.field_names=['SKlearn','BGD', 'SGD', 'Difference']\n",
    "\n",
    "# You can directly run the code below to output the table or rewrite it.\n",
    "# Please remain five decimal places\n",
    "for i in range(30):\n",
    "    p.add_row(['{:.5f}'.format(LRclf.coef_[0,i]),'{:.5f}'.format(w_BGD[i]), \n",
    "               '{:.5f}'.format(w_SGD[i]), '{:.5f}'.format(abs(w_BGD[i]-w_SGD[i]))])\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c12024a3da92c928c2602c95d33715e120a7363e89ff3984359dd244363bf7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
